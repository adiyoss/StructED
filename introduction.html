<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="StructED, structured prediction learning package">
    <meta name="author" content="adiyoss yossi adi joseph keshet">
    <link rel="icon" href="../../favicon.ico">

    <title>StructED - Introduction</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

   <!-- Custom styles for this template -->
    <link href="css/navbar-fixed-top.css" rel="stylesheet">
    <link href="css/sticky-footer-navbar.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/jumbotron.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="assets/ie-emulation-modes-warning.js"></script>

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">

    <link href="css/introduction.css" rel="stylesheet">  

    <!-- USED FOR LATEX CODE INSIDE THE HTML -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
      });
    </script>
    <script type="text/javascript" src="js/MathJax.js"></script>  

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">StructED</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">            
            <li><a href="introduction.html">Introduction</a></li>
            <li><a href="algorithms.html">Algorithms</a></li>
            <li><a href="api/index.html" target="blank">API</a></li>
            <li><a href="examples.html">Examples</a></li>
            <li><a href="contact.html">Contact</a></li>            
            <li><a href="https://github.com/adiyoss/StructED" target="blank"><i class="fa fa-github fa-fw fa-lg"></i></a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div style="display:none">
    \(      
      \newcommand{\oneinf}{\ell_{1,\infty}}
      \newcommand{\onetwo}{\ell_{1,2}}
    \)
    </div>
    

    <div class="container container_short">
      <!-- Example row of columns -->
      <h2 class="text-center">Introduction</h2>
      <hr>
      <div> 
          <h3><u>What is Structured Prediction?</u></h3> 
          <div>
            <p>The ultimate objective of discriminative learning is to train a system to optimize a desired measure of performance. In binary classification we are interested in finding a function that assigns a binary label to a single object, and minimizes the error rate (correct or incorrect) on unseen data.</p> 
            <p>In structured prediction the input is a complex object (a spoken utterance, an image, a phrase, a graph, etc.), and we are interested in the prediction of a structured label (a sequence of words, a bounding box, a parsing tree, etc.). Typically, each structured prediction task has its own measure of performance or evaluation metric, such as word error rate in speech recognition, the BLEU score in machine translation or the intersection-over-union score in object segmentation.</p>
          </div>          
          <div> <u>Example</u> In the problem of vowel duration measurement, our goal is to predict the start and end times of a vowel phoneme in a spoken word. One approach is to simply train a binary classifier at the frame level to predict for each frame weather it is a vowel or not. This approach does not take into consideration the internal structure of the desire label and relation between its components, such as typical vowel duration, or its temporal and spectral characteristics. In contrary, the structured prediction approach can look at the desired label as one piece and define a set of features maps that captures the relations between target internal components. A typical feature can be the presume duration of the vowel relative to the average vowel duration. We provide very detailed tutorial on the <i>vowel duration measurement</i> which can be found <a href="vowel.html">here.</a>
          </div><br>
          <div>
            We begin by posing the structured learning setting. We consider a supervised learning setting with input objects $x \in \mathcal{X}$ and target labels $y \in \mathcal{Y}$. The labels may be sequences, trees, grids, or other high-dimensional objects with internal structure. We assumed a fixed mapping $\phi: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ from the set of input objects and target labels to a real vector of length $d$. We call the elements of this mapping feature functions/feature maps.
          </div><br>
          <div> 
            <u>Example</u> In the problem of pronunciation modeling, the input $x$ is a pronounced sequence of phones, and the output $y$ is the pronounced word. Most often pronunciation variations cannot be found in a lexicon of canonical pronunciation, and have to be modeled statistically. In this task, the feature functions map a pronunciation from a set of pronunciations of different lengths along with a proposed word to a vector of fixed dimension in $\mathbb{R}^d$. One such feature function might measure the edit distance between the pronunciation $x$ and the canonical pronunciation of the word $y$ in the lexicon. This feature function counts the minimum number of edit operations (insertions, deletions, and substitutions) that are needed to convert the actual pronunciation to the lexicon pronunciation; it is low if the actual pronunciation is close to the lexicon and high otherwise. See <a href="http://www.cs.biu.ac.il/~jkeshet/papers/TangKeLi12.pdf">Hao, Keshet, and Livescu (2012)</a> and the references therein for full description of this task and other examples of feature functions.
          </div><br>

          </ol> 
          <h3><u>More Formally</u></h3>          
          <div>
            <p>Consider a supervised learning setting with input instances $x \in \mathcal{X}$ and target labels $y \in \mathcal{Y}$, which refers to a set of complex objects with an internal structure. We assumed a fixed mapping called feature functions (or feature maps) $\phi: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ from the set of input objects and target labels to a real vector of length $d$. Also consider a linear decoder with parameters $w \in \mathbb{R}^d$, such that $\hat{y}$ is a good approximation to the true label of $x$, as follows:</p>
            <p class="text-center">              
              \begin{equation}
              \label{eq:decoding}
              \hat{y} = argmax_{y \in \mathcal{y}} ~ w^\top \phi(x, y)
              \end{equation}    
            </p>
            
            <p>Ideally, the learning algorithm finds $w$ such that the prediction rule optimizes the expected desired $\textit{measure of preference}$ or $\textit{evaluation metric}$ on unseen data. We define a $\textit{cost}$ function, $\ell(y, \hat{y})$, to be a non-negative measure of error when predicting $\hat{y}$ instead of $y$ as the label of $x$. Our goal is to find the parameters vector $w$ that minimizes this function. Often the desired evaluation metric is a utility function that needs to be maximized (like BLEU or NDCG) and then we define the cost to be 1 minus the evaluation metric. We assume that exists some unknown probability distribution $\rho$ over pairs $(x,y)$ where $y$ is the desired output for input $x$. We would like to set $w$ so as to minimize the expected cost, or the $\textit{risk}$, for predicting $\hat{y}$,</p>

            <p class="text-center">
              \begin{equation}
              \label{eq:w*}
              w^* = argmin_{w} ~ \mathbb{E}_{(x,y) \sim \rho} [\ell(y,\hat{y}(x))]. 
              \end{equation}
            </p>
            
            <p>This objective function is hard to minimize directly <a href="http://u.cs.biu.ac.il/~jkeshet/papers/Keshet14.pdf" target="blank">(Keshet, 2014)</a>. Given a training set of examples $\mathcal{S} = \{(x_i,y_i)\}_{i=1}^{m}$, where each pair $(x_i, y_i)$ is drawn i.i.d from $\rho$, a common practice is to find the model parameters that minimize the regularized mean surrogate loss, </p>
            
            <p class="text-center">
              \begin{equation}
              \label{eq:reg-loss}
              w^* = argmin_{w}  ~ \frac{1}{m}\sum_{i=1}^{m} \bar{\ell}(w,x_i,y_i) + \frac{\lambda}{2} \|w\|^2, 
              \end{equation} 
            </p>

            <p>where $\bar{\ell}(w,x,y)$ is a surrogate loss function, and $\lambda$ is a trade-off parameter between the loss term and the regularization. Each algorithm has its own definition of the surrogate loss, e.g., the surrogate loss in max-margin Markov model <a href="http://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf" target="blank">(Taskar et al., 2003)</a> is the structured hinge loss with the Hamming cost, whereas the surrogate loss in conditional random fields <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf" target="blank">(Lafferty et al., 2001)</a> is the log loss function. A general survey on structured prediction algorithms and their prediction rules is given in <a href="http://u.cs.biu.ac.il/~jkeshet/papers/Keshet14.pdf" target="blank">(Keshet, 2014)</a>.
            </p>            
          </div>

          <h3><u>Modules</u></h3>  
            <p>
              Let us first define the loss augmented function as follows:
              <p class="text-center">              
                \begin{equation}
                \label{eq:loss-augmented}
                \hat{y}^{\epsilon} = argmax_{\hat{y} \in \mathcal{Y}} ~ w^\top \phi(x, \hat{y}) + \epsilon \, \ell (y,\hat{y})
                \end{equation}    
              </p>
              where epsilon can be 0 if we wish to use a standard linear decoder, 1 if we wish to use a loss-augmented or any other $\epsilon$ value (as used in DLM).
            </p>

          <p>We implemented six structured prediction algorithms in StructED, all the loss functions and update rules are summarized in the following table:</p>         
          <div>
            <table class="table table-bordered table-striped">             
             <thead>
                <tr>                                    
                  <th class="col-md-5">Loss</th>
                  <th class="col-md-5">Update Rule</th>
                </tr>
             </thead>
             <tbody>
                <tr>
                  <td colspan="2"><b>Structured Perceptron</b> <a href="http://www.cs.columbia.edu/~mcollins/papers/tagperc.pdf" target="blank">(Collins, 2002).</td>
                </tr>
                <tr>                   
                   <td class="text-center"> - </td>
                   <td>$w_{t+1} = w_t + \phi(x_t, y_t) - \phi(x_t,\hat{y}^0_t)$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Structured SVM</b> <a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf" target="blank">(Tsochantaridis et al., 2005).</td>
                </tr>
                <tr>                   
                   <td class="text-center">$max_{\hat{y}} ~[ \ell(y,\hat{y}) - w^\top\phi(x,y) + w^\top\phi(x,\hat{y})]$</td>
                   <td>$w_{t+1} = (1-\eta_t \lambda)w_t + \eta_t( \phi(x_t, y_t) - \phi(x_t,\hat{y}^1_t))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Passive Aggressive</b> <a href="http://webee.technion.ac.il/people/koby/publications/crammer06a.pdf" target="blank">(Crammer et al., 2006).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center"> - </td>
                   <td>$w_{t+1} = w_{t} + \tau_{t}(\phi(x_t,y_t)) - \phi(x_t,\hat{y}^1_t))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>CRF</b> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf" target="blank">(Lafferty et al., 2001).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$-\ln P_{w}(y \,|\, x)$, where $P_{w}(y \,|\, x) = \frac{1}{Z_{w}(x)} \exp\{w^\top \phi(x, y)\}$</td>
                   <td>$w_{t+1} = (1-\eta_t \lambda)\,w_{t} + \eta_t\Big( \phi(x_{j_t},y_{j_t}) - \mathbb{E}_{y'\sim P_{w}(y' \,|\, x)}[ \phi(x_{j_t},y')] \Big)$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Direct Loss Minimization</b> <a href="http://u.cs.biu.ac.il/~jkeshet/papers/McAllesterHaKe10.pdf" target="blank">(McAllester et al., 2010).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$\ell (y,\hat{y})$</td>
                   <td>$w_{t+1} = w_{t} + \frac{\eta_t}{\epsilon}(\phi(x_t,\hat{y}^{-\epsilon}_t) - \phi(x_t,\hat{y}^0_{t}))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Structured Ramp Loss</b> <a href="http://papers.nips.cc/paper/4268-generalization-bounds-and-consistency-for-latent-structural-probit-and-ramp-loss.pdf" target="blank">(McAllester and Keshet, 2011).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$\max_{\hat{y}} [ \ell(y,\hat{y}) +  w^\top \phi(x,\hat{y})] - \max_{\tilde{y}} [w^\top \phi(x,\tilde{y})]$</td>
                   <td>$w_{t+1} = (1-\eta_t\lambda)\,w_{t} + \eta_t(\phi(x_t,\hat{y}^0_t) - \phi(x_t,\hat{y}^1_t))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Probit Loss</b> <a href="http://cs.haifa.ac.il/~tamir/papers/KeshetMcHa10.pdf" target="blank">(Keshet et al., 2011).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$\mathbb{E}_{\gamma \sim \mathcal{N}(0,I)}[\ell (y,\hat{y}_{w+\gamma})]$</td>
                   <td>$w_{t+1} = (1-\eta_t\lambda)\,w_t + \eta_t \mathbb{E}_{\gamma \sim \mathcal{N}(0,I)}[ \gamma \, \ell (y_t,\hat{y}^0_{w+\gamma})]$</td>
                </tr>

             </tbody>
          </table>
          </div>
          <div>
            <p>Although it is not so popular in structured prediction, we also implemented two kernel expansion functions:</p>
            <ol>
              <li>Polynomial.</li>
              <li>RBF using 2nd and 3rd Taylor approximation <a href="http://arxiv.org/abs/1109.4603v1">(Cotter, Keshet and Srebro, 2011)</a></li>
            </ol>
            <p>Moreover, in order to support multi-class, we also implemented the feature functions, inference, loss-augmented inference and and use zero-one loss as a cost function. Hence, using StructED as a multi-class classifier is straightforward. More details about using StructED for multi-class tasks can be found <a href="multiclass.html">here.</a></p>
          </div>          
      </div>
    </div> <!-- /container -->    

    <footer class="footer">
      <div class="container">
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="assets/ie10-viewport-bug-workaround.js"></script>    
  </body>
</html>
