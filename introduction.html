<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="StructED, structured prediction learning package">
    <meta name="author" content="adiyoss yossi adi joseph keshet">
    <link rel="icon" href="../../favicon.ico">

    <title>StructED - Introduction</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

   <!-- Custom styles for this template -->
    <link href="css/navbar-fixed-top.css" rel="stylesheet">
    <link href="css/sticky-footer-navbar.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/jumbotron.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="assets/ie-emulation-modes-warning.js"></script>

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">

    <link href="css/introduction.css" rel="stylesheet">  

    <!-- USED FOR LATEX CODE INSIDE THE HTML -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
      });
    </script>
    <script type="text/javascript" src="js/MathJax.js"></script>  

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">StrcutED</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">            
            <li class="active"><a href="introduction.html">Introduction</a></li>
            <li><a href="installation.html">Installation</a></li>
            <li><a href="tut_general.html">Tutorials</a></li>
            <li><a href="https://github.com/adiyoss/StructED/issues" target="blank">Issues</a></li>
            <li><a href="contact.html">Contact</a></li>
            <li><a href="https://github.com/adiyoss/StructED" target="blank"><i class="fa fa-github fa-fw fa-lg"></i></a></li>
          </ul>
        </div><!--/.nav-collapse -->
    </nav>

    <div style="display:none">
    \(      
      \newcommand{\oneinf}{\ell_{1,\infty}}
      \newcommand{\onetwo}{\ell_{1,2}}
    \)
    </div>
    

    <div class="container container_short">
      <!-- Example row of columns -->
      <h2 class="text-center">Introduction</h2>
      <hr>
      <div> 
          <h3><u>What is Structred Prediction?</u></h3> 
          <div>
            <p>The ultimate objective of discriminative learning is to train a system to optimize a  desired measure of performance. In binary classification we are interested in finding a function that assigns a binary label to a single object, and minimizes the error rate (correct or incorrect) on unseen data.</p> 
            <p>In structured prediction the input is a complex object (a spoken uttreance, an image, a phrase, a graph, etc.), and we are interested in the prediction of a structured label (a sequence of words, a bounding box, a parsing tree, etc.). Typically, each structured prediction task has its own measure of performance or evaluation metric, such as word error rate in speech recognition, the BLEU score in machine translation or the intersection-over-union score in object segmentation.</p>
          </div>          
          <div> <u>Example</u>
            In the problem of vowel duration measurement, we wish to measure the start and end times of a vowel phoneme in a spoken word. We could simply train a binary classifier that train to predict if a given 10 msec frame of speech is a vowel or a consonant, and then use it to predict wheather each frames of the test utterance is vowel or not. The we could just run on all the signal frames and mark the start and end time of the vowel. This approach, however,  does not take into consideration the internal structure of the desire label, such as typical vowel duration, or its temporal and spectral characteristics. The structured prediction approach can look at the desired label as a whole and define a set of features that dependes on it onset, it offset or the relation between them. Atypical feature can be the presume duration of the vowel relative to the avarage vowel duration. We provide very detailed tutorial on the <i>vowel duration measurement</i> which can be found <a href="tut_general.html">here.</a>
          </div><br>
          <div>
            We begin by posing the structured learning setting. We consider a supervised learning setting with input objects $x \in \mathcal{X}$ and target labels $y \in \mathcal{Y}$. The labels may be sequences, trees, grids, or other high-dimensional objects with internal structure. We assumed a fixed mapping $\phi: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ from the set of input objects and target labels to a real vector of length $d$. We call the elements of this mapping feature functions.
          </div><br>
          <div> 
            <u>Example</u> In the problem of pronunciation modeling, the input $x$ is a pronounced sequence of phones, and the output $y$ is the pronounced word. Most often pronunciation variations cannot be found in a lexicon of canonical pronunciation, and have to be modeled statistically. In this task, the feature functions map a pronunciation from a set of pronunciations of different lengths along with a proposed word to a vector of fixed dimension in $\mathbb{R}^d$. One such feature function might measure the edit distance between the pronunciation $x$ and the canonical pronunciation of the word $y$ in the lexicon. This feature function counts the minimum number of edit operations (insertions, deletions, and substitutions) that are needed to convert the actual pronunciation to the lexicon pronunciation; it is low if the actual pronunciation is close to the lexicon one and high otherwise. See <a href="http://www.cs.biu.ac.il/~jkeshet/papers/TangKeLi12.pdf">Hao, Keshet, and Livescu (2012)</a> and the references therein for full description of this task and other examples of feature functions.
          </div><br>

          </ol> 
          <h3><u>More Formally</u></h3>          
          <div>
            <p>Consider a supervised learning setting with input instances $x \in \mathcal{X}$ and target labels $y \in \mathcal{Y}$, which refers to a set of complex objects with an internal structure. We assumed a fixed mapping called feature functions (or feature maps) $\phi: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ from the set of input objects and target labels to a real vector of length $d$. Also consider a linear decoder with parameters $w \in \mathbb{R}^d$, such that $\hat{y}$ is a good approximation to the true label of $x$, as follows:</p>
            <p class="text-center">              
              \begin{equation}
              \label{eq:decoding}
              \hat{y} = argmax_{y \in \mathcal{y}} ~ w^\top \phi(x, y)
              \end{equation}    
            </p>
            
            <p>Ideally, the learning algorithm finds $w$ such that the prediction rule optimizes the expected desired $\textit{measure of preference}$ or $\textit{evaluation metric}$ on unseen data. We define a $\textit{cost}$ function, $\ell(y, \hat{y})$, to be a non-negative measure of error when predicting $\hat{y}$ instead of $y$ as the label of $x$. Our goal is to find the parameters vector $w$ that minimizes this function. Often the desired evaluation metric is a utility function that needs to be maximized (like BLEU or NDCG) and then we define the cost to be 1 minus the evaluation metric. We assume that exists some unknown probability distribution $\rho$ over pairs $(x,y)$ where $y$ is the desired output for input $x$. We would like to set $w$ so as to minimize the expected cost, or the $\textit{risk}$, for predicting $\hat{y}$,</p>

            <p class="text-center">
              \begin{equation}
              \label{eq:w*}
              w^* = argmin_{w} ~ \mathbb{E}_{(x,y) \sim \rho} [\ell(y,\hat{y}(x))]. 
              \end{equation}
            </p>
            
            <p>This objective function is hard to minimize directly <a href="http://u.cs.biu.ac.il/~jkeshet/papers/Keshet14.pdf" target="blank">(Keshet, 2014)</a>. Given a training set of examples $\mathcal{S} = \{(x_i,y_i)\}_{i=1}^{m}$, where each pair $(x_i, y_i)$ is drawn i.i.d from $\rho$, a common practice is to find the model parameters that minimize the regularized mean surrogate loss, </p>
            
            <p class="text-center">
              \begin{equation}
              \label{eq:reg-loss}
              w^* = argmin_{w}  ~ \frac{1}{m}\sum_{i=1}^{m} \bar{\ell}(w,x_i,y_i) + \frac{\lambda}{2} \|w\|^2, 
              \end{equation} 
            </p>

            <p>where $\bar{\ell}(w,x,y)$ is a surrogate loss function, and $\lambda$ is a trade-off parameter between the loss term and the regularization. Each algorithm has its own definition of the surrogate loss, e.g., the surrogate loss in max-margin Markov model <a href="http://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf" target="blank">(Taskar et al., 2003)</a> is the structured hinge loss with the Hamming cost, whereas the surrogate loss in conditional random fields <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf" target="blank">(Lafferty et al., 2001)</a> is the log loss function. A general survey on structured prediction algorithms and their prediction rules is given in <a href="http://u.cs.biu.ac.il/~jkeshet/papers/Keshet14.pdf" target="blank">(Keshet, 2014)</a>.
            </p>            
          </div>

          <h3><u>Modules</u></h3>  
            <p>
              Let us first define the loss augmented function as follows:
              <p class="text-center">              
                \begin{equation}
                \label{eq:loss-augmented}
                \hat{y}^{\epsilon} = argmax_{\hat{y} \in \mathcal{Y}} ~ w^\top \phi(x, \hat{y}) + \epsilon \, \ell (y,\hat{y})
                \end{equation}    
              </p>
              where epsilon can be 0 if we wish to use a standard linnear decoder, 1 if we wish to use a loss-augmented or any other $\epsilon$ value (as used in DLM).
            </p>

          <p>We implemented six structured prediction algorithms in StructED, all the loss functions and update rules are summerized in the following table:</p>         
          <div>
            <table class="table table-bordered table-striped">             
             <thead>
                <tr>                                    
                  <th class="col-md-5">Loss</th>
                  <th class="col-md-5">Update Rule</th>
                </tr>
             </thead>
             <tbody>
                <tr>
                  <td colspan="2"><b>Structured Perceptron</b> <a href="http://www.cs.columbia.edu/~mcollins/papers/tagperc.pdf" target="blank">(Collins, 2002).</td>
                </tr>
                <tr>                   
                   <td class="text-center"> - </td>
                   <td>$w_{t+1} = w_t + \phi(x_t, y_t) - \phi(x_t,\hat{y}^0_t)$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Structured SVM</b> <a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf" target="blank">(Tsochantaridis et al., 2005).</td>
                </tr>
                <tr>                   
                   <td class="text-center">$max_{\hat{y}} ~[ \ell(y,\hat{y}) - w^\top\phi(x,y) + w^\top\phi(x,\hat{y})]$</td>
                   <td>$w_{t+1} = (1-\eta_t \lambda)w_t + \eta_t( \phi(x_t, y_t) - \phi(x_t,\hat{y}^1_t))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Passive Aggressive</b> <a href="http://webee.technion.ac.il/people/koby/publications/crammer06a.pdf" target="blank">(Crammer et al., 2006).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center"> - </td>
                   <td>$w_{t+1} = w_{t} + \tau_{t}(\phi(x_t,y_t)) - \phi(x_t,\hat{y}^1_t))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>CRF</b> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf" target="blank">(Lafferty et al., 2001).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$-\ln P_{w}(y \,|\, x)$, where $P_{w}(y \,|\, x) = \frac{1}{Z_{w}(x)} \exp\{w^\top \phi(x, y)\}$</td>
                   <td>$w_{t+1} = (1-\eta_t \lambda)\,w_{t} + \eta_t\Big( \phi(x_{j_t},y_{j_t}) - \mathbb{E}_{y'\sim P_{w}(y' \,|\, x)}[ \phi(x_{j_t},y')] \Big)$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Direct Loss Minimization</b> <a href="http://u.cs.biu.ac.il/~jkeshet/papers/McAllesterHaKe10.pdf" target="blank">(McAllester et al., 2010).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$\ell (y,\hat{y})$</td>
                   <td>$w_{t+1} = w_{t} + \frac{\eta_t}{\epsilon}(\phi(x_t,\hat{y}^{-\epsilon}_t) - \phi(x_t,\hat{y}^1_{t}))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Structured Ramp Loss</b> <a href="http://papers.nips.cc/paper/4268-generalization-bounds-and-consistency-for-latent-structural-probit-and-ramp-loss.pdf" target="blank">(McAllester and Keshet, 2011).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$\max_{\hat{y}} [ \ell(y,\hat{y}) +  w^\top \phi(x,\hat{y})] - \max_{\tilde{y}} [w^\top \phi(x,\tilde{y})]$</td>
                   <td>$w_{t+1} = (1-\eta_t\lambda)\,w_{t} + \eta_t(\phi(x_t,\hat{y}^0_t) - \phi(x_t,\hat{y}^1_t))$</td>
                </tr>
                <tr>
                  <td colspan="2"><b>Probit Loss</b> <a href="http://cs.haifa.ac.il/~tamir/papers/KeshetMcHa10.pdf" target="blank">(Keshet et al., 2011).</a></td>
                </tr>
                <tr>                   
                   <td class="text-center">$\mathbb{E}_{\gamma \sim \mathcal{N}(0,I)}[\ell (y,\hat{y}_{w+\gamma})]$</td>
                   <td>$w_{t+1} = (1-\eta_t\lambda)\,w_t + \eta_t \mathbb{E}_{\gamma \sim \mathcal{N}(0,I)}[ \gamma \, \ell (y_t,\hat{y}^0_{w+\gamma})]$</td>
                </tr>

             </tbody>
          </table>
          </div>
          <div>
            <p>Although it is not so populare in structured prediction, we also implemented two kernel expansion functions:</p>
            <ol>
              <li>Polynomial.</li>
              <li>RBF using 2nd and 3rd Taylor approximation <a href="http://arxiv.org/abs/1109.4603v1">(Cotter, Keshet and Srebro, 2011)</a></li>
            </ol>
            <p>Moreover, in order to support multi-class, we also implemented the feature functions, the inference, the loss-augmented inference and and use the zero-one loss as a cost function. Hence, using StructED as a multi-class classifier is straightforward. More detailes about using StructED for multi-class tasks can be found <a href="tut_general.html">here.</a></p>
          </div>          
      </div>
    </div> <!-- /container -->    

    <footer class="footer">
      <div class="container">
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="assets/ie10-viewport-bug-workaround.js"></script>    
  </body>
</html>
